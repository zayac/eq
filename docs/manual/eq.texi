\input texinfo   @c -*-texinfo-*-
@c %**start of header
@setfilename eq.info
@include macros.texi
@settitle Using the @eq{} language
@c %**end of header

@copying
@c man begin COPYRIGHT
Copyright @copyright{} 2011,2012 
        Artem Shinkarov @email{artyom.shinkaroff@@gimail.com}
        Pavel Zaichenkov @email{zaichenkov@@gmail.com}

Permission to use, copy, modify, and distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
@c man end
@end copying

@titlepage
@title Using the @eq{} language
@author Pavel Zaichenkov, Artem Shinkarov

@page
@vskip 0pt plus 1filll

@c Published by ...
@end titlepage

@c This is a raw html options to control the generated html with jquery.
@html
<script src="jquery.js"></script>
<script src="fixes.js"></script>
@end html


@c So the toc is printed at the start.
@contents

@node Top, Eq -- big picture
@top Introduction

This manual describes how to use @eq{} compiler and tools, their
features and incompatibilities.

@menu
* Eq -- big picture::  Eq is a high performance stream processing language.
* Language specification:: Eq programming language specification.
* Compiler usage::   Compiling progrmans with eq2c.
* Implementation details::  Anatomy of the compiler.
* Known problems::   Problems with Eq compiler.
* Reporting bugs::   How to report a bug.
* Getting help::     How to get help regarding Eq.
* Contributing to Eq:: How to contribute to the project.
* Keyword index::    Index of symbol names.
* Option index::     Index of the options of the compiler.
@end menu

@node Eq -- big picture
@chapter Eq -- big picture

@cindex Eq
@cindex functional language
@cindex stream processing
Eq is a stream-based functional language designed for solving numerical tasks.
The syntax of the Eq programming language is based on a document preparation
system LaTeX. It follows that source files of the Eq can be interpreted by any
LaTeX conversion tool (i.e. pdflatex, latex2html, etc).


@ignore
@c man begin SYNOPSIS
@eq{} [@option{-help}] [@option{-I}@var{dir}@dots{}]

The most useful options are listed here; see below for the remainder.
@c man end

@c man begin SEEALSO
gcc(1) cpp(1) ld(1) as(1)
@c man end

@c man begin BUGS
For more details on reporting bugs, see
@w{@uref{http://github.com/zayac/eq/issues}}
@c man end BUGS

@c man begin AUTHOR
The language is currently developed by Pavel Zaichenkov and Artem
Shinkarov.
@c man end AUTHOR
@end ignore



@node Language specification
@chapter Language specification

@fixme{Describe eq syntax and semantics}

@menu
* Lexical elements
* Constants
* Types
* Blocks
* Declarations and scope
* Expressions
* Statements
@end menu

@section Lexical elements
Source code is ANSI text. The language is case-sensitive, i.e. upper and lower
case letters are different characters.

As a source code can be parsed by LaTeX compiler, lexical structures in Eq look
very similar to those in LaTeX.

@subsection Letters and digits
@lisp
letter        = "A" ... "Z" | "a" ... "z"
decimal_digit = "0" ... "9"
hex_digit     = "0" ... "9" | "a" ... "f" | "A" ... "F"
@end lisp

@subsection Comments
There is two forms of comment:
@enumerate
  @cindex line comment
  @item @emph{Line comment} -- starts from the character @code{%} and stops at
  the end of the line. A line comment acts as a new line. This comment is
  interpreted as a comment by LaTeX compiler as well.
  @cindex visual comment
  @item @emph{visual comment} -- starts with the keyword @code{\comment} which
  is followed by a string in braces. This comment is interpreted as a text by
  LaTeX compiler and will be printed in the final document.
@end enumerate

@subsection Tokens
There are six basic token classes: @dfn{keyword}, @dfn{identifier},
@dfn{operator}, @dfn{real number}, @dfn{integer number}, @dfn{string},
@dfn{white space}, @dfn{comment} and @dfn{EOF}.
@code{EOF} character is the only one which has @dfn{EOF} type. Every input file
is parsed until @code{EOF} character is occured.

@subsection Identifiers
An identifier as a sequence of one or more letters and digits.
@lisp
identifier = letter @{ letter | digit | "\_" @}
@end lisp

A set of certain keywords can be used as identifiers as well.

@subsection Keywords
The following keywords are reserved and may not be used as identifiers: @dfn{B},
@dfn{N}, @dfn{R}, @dfn{Z}, @dfn{\arraytype}, @dfn{\begin}, @dfn{\call},
@dfn{\comment'}, @dfn{\dfrac'}, @dfn{\end'}, @dfn{\endl}, @dfn{\expr},
@dfn{\filter}, @dfn{\forall}, @dfn{\frac}, @dfn{\genar},  @dfn{\hex},
@dfn{\iif}, @dfn{\in}, @dfn{\land}, @dfn{\ldots}, @dfn{\lend}, @dfn{\leq},
@dfn{\limits}, @dfn{\ll}, @dfn{\lnot}, @dfn{\lor}, @dfn{\match}, @dfn{\opt},
@dfn{\otherwise}, @dfn{\overline}, @dfn{\print}, @dfn{\proto}, @dfn{\qelse},
@dfn{\qelseif}, @dfn{\qendif}, @dfn{\qif}, @dfn{\return}, @dfn{\sum},
@dfn{\to}, @dfn{\type}, @dfn{cases}, @dfn{eqcode}, @dfn{tmatrix},
@dfn{tvector}.  

Another set of keywords may be used as identifiers. Basically these are
keywords representing greek alphabet letters: @dfn{\alpha}, @dfn{\beta},
@dfn{\chi}, @dfn{\delta}, @dfn{\epsilon}, @dfn{\eta}, @dfn{\gamma},
@dfn{\iota}, @dfn{\kappa}, @dfn{\lambda}, @dfn{\mu}, @dfn{\omega}, @dfn{\phi},
@dfn{\pi}, @dfn{\psi}, @dfn{\rho}, @dfn{\sigma}, @dfn{\tau}, @dfn{\theta},
@dfn{\upsilon}, @dfn{\xi}, @dfn{\zeta}, @dfn{\iter}.

The last keyword @dfn{\iter} represents an recurrence index of recurrence
equation. This can be used only in certain cases:
@cindex \iter
@enumerate
  @item in the left part of recurrence equation when providing a recurrent
  formula;
  @item in the right part of recurrence equation when we are giving a defintion
  of a recurrent formula;
  @item inside filter expression predicate;
@end enumerate

It is wrong to use @dfn{\iter} variable when:
@enumerate
  @item assigning to a non-recurrence variable;
  @item providing a list of function arguments;
  @item in recurrence equation defining base cases;
@end enumerate

The set of keywords can be extended using macroprocessor.

@subsection Operators and Delimiters
The following tokens are considered as operators, delimiters and other special
tokens: @dfn{\cap}, @dfn{\cdot},
@dfn{\cup}, @dfn{\geq}, @dfn{\gets}, @dfn{\gg}, @dfn{\in}, @dfn{\land},
@dfn{\leq}, @dfn{\ll}, @dfn{\lnot}, @dfn{\lor}, @dfn{\mod}, @dfn{\neq},
@dfn{\not}, @dfn{\oplus}, @dfn{&}, @dfn{+}, @dfn{|}, @dfn{-}, @dfn{=}, @dfn{^},
@dfn{_}, @dfn{>}, @dfn{<}, @dfn{,}, @dfn{(}, @dfn{)}, @dfn{[}, @dfn{]},
@dfn{@{}, @dfn{@}}, @dfn{:}, @dfn{;}.

@subsection Whitespaces
These character sequences represent whitespaces:
@dfn{\,}, @dfn{\:}, @dfn{\;}, @dfn{ }, @dfn{\qquad}, @dfn{\quad}.

Basically they are used for visual formatting only, and don't have a semantic
meaning.

@subsection Integer literals
@lisp
int_lit     = decimal_lit | hex_lit
decimal_lit = ( "1" ... "9" ) @{ decimal_digit @}
hex_lit     = "\hex" "@{"  hex_digit @{ hex_digit @} "@}"
@end lisp

@subsection Floating-point literals
@lisp
float_lit = decimals "." [ decimals ] [ exponent ] |
            decimals exponent |
            "." decimals [ exponent ]
decimals  = decimal_digit @{ decimal_digit @}
exponent  = ( "e" | "E" ) [ "+" | "-" ] decimals
@end lisp

@subsection String literals
@fixme{At the moment string literals are supported by lexer only. Need to
construct semantics for working with them.}

@node Compiler usage
@chapter Compiling with @eq{}

@menu
* Option Summary::	Summarizes the options available.
* Overall options::	General options to set-up compilation environment.
* Print options::	General options to print a state of AST.
* Break options::	Options to break the compiler.
@end menu

@c man begin DESCRIPTION
The @eq compiler compiles from a latex-based syntax files into the
executables.

Blah blah..

@c man end


@c man begin OPTIONS
@node Option Summary
@section Option Summary

@table @emph
@item Overall options
@xref{Overall options,,General options}.
@optlist{-V}


@item Print options
@xref{Print options}.
@optlist{-P @r{[}@var{program types matches}@r{]}}

@item Break options
@xref{Break options}.
@optlist{-B @var{spec}}

@end table



@node Overall options
@section General options

When one of the following options is called @eq{} does not perform
any compilation.

@cindex overall optiona
@table @tabopt

@item -V
@opindex V
Print the version of the compiler.

@end table


@node Print options
@section Print options
These options are used to print a state of the program after a certain
phase of the compiler.

@table @tabopt

@item -P @var{phase}
@opindex P


@var{phase} is a name of the phase after which compiler would stop
its execution and print the state of the abstract syntax tree to 
standard output.

@itemize
@item @b{program}
stop after the parsing phase.
@item @b{types}
stop after the type-checking phase.
@item @b{matches}
stop after used-defined matches resolution.
@end itemize
@end table


@node Break options
@section Break options
Break options allow you to stop the compilation process after a
particular phase.

@cindex break options
@table @tabopt

@item -B @var{phase}
@opindex B 
Break after a phase defined by @var{phase}, which can have the following
values.

@itemize
@item @b{parser}
Stop after the parsing phase.
@end itemize
@end table
@c man end


@node Implementation details
@chapter Implementation details

@section Lexer
Lexer parses input text file(s), converting a sequence of characters into
sequence of tokens. The main work takes place inside @file{lex.c}.
@example
struct eq_token
@{
  struct eq_location loc;
  enum eq_token_class tok_class;
  bool uses_buf;
  union
  @{
    char *cval;
    enum eq_token_kind tval;
  @} value;
@};
@end example
This is a structure to store token value. Each token is associated with a
class. A list of classes can be found inside @file{token_class.def}. Tokens are
divided into two categories: those, that use additional memory, and those that
don't. Identifier @dfn{tok_id} and unknown tokens @dfn{tok_unknown} use
additional memory to store it's literal name. If extra memory is not used, than
a value from @dfn{eq_token_kind_name} array is associated with the token.
Values for tokens are defined in @file{token_kind.def} (for operators) and in
@file{keywords.def} (for operators and keywords). @code{eq_lexer_get_token}
function reads the next sequence of characters from input file, constructs and
returns a new token structure.

These methods of lexer module are public:
@itemize
@item
@code{bool eq_lexer_init (struct eq_lexer *, const char *)} -- initialize lexer 
structure with a given file name. Initial parameters are set.

@item
@code{bool eq_lexer_finalize (struct eq_lexer *)} -- actions to perform before
deallocating lexer. This doesn't deallocate lexer structure!

@item
@code{bool eq_is_id (struct eq_token *, bool)} -- check either token is a valid
identifier. The second argument indicates either error needs to be shown in
case token is not a valid id.

@item
@code{bool eq_token_is_delimiter (struct eq_token *)} -- checks either token is
a valid delimiter. A list of delimiters can be found in @file{delimiters.def}.
Delimiters are introduced for usage with LaTeX @dfn{\left} and @dfn{\right}
keywords. If a sequence of tokens @code{\left <delimiter_token>} or
@code{\right <delimiter_token>} is found, then the first token should be
ignored by parser, as it affects only visual appearance in LaTeX.

@item
@code{struct eq_token *eq_lexer_get_token (struct eq_lexer *)} -- the main
lexer function which reads the next sequence of characters from input file,
constructs and returns a new token.

@item
@code{struct eq_token *eq_token_copy (struct eq_token *)} -- copies a token
allocating new memory. A memory for a string is copied if necessary too.

@item
@code{int eq_token_compare (struct eq_token *, struct eq_token *)} -- compares
two tokens. It doesn't take into consideration token locations.

@item
@code{void eq_token_free (struct eq_token *)} -- deallocate the memory that
token occupies. If token uses additional memory for string, it is deallocated
too.

@item
@code{void eq_token_print (struct eq_token *)} -- print token. Used mainly for
debug output.

@item
@code{const char *eq_token_as_string (struct eq_token *)} -- return
string that represents token value (works for tokens with and without
additional memory used).

@item
@code{bool eq_token_uses_buf (struct eq_token *)} -- indicate either token uses
extra memory.
@end itemize

@section Parser
A parser builds an hierarchical tree structure from sequence of tokens. The
tree structure is predefined by grammar rules which are described in
@file{parser.c}. During parsing a macroprocessor is used as well.

The parser calls @code{eq_lexer_get_token} function until EOF token found. All
recent tokens are stored in token buffer @code{token_buffer} inside
@code{struct eq_parser} structure. Physically token is parsed and put into
token array once only. When @code{eq_parser_unget} is called, then the array
index pointer is decreased, when @code{eq_parser_get_token} is called again,
then the pointer is increased, and a corresponding value from the buffer is
returned. It should be noticed that the buffer size @code{buf_size} could
affect parsing correctness, so that not more than @code{buf_size} tokens can be
looked ahead.

Compiler supports hexidecimal numbers only on the stage of lexer. The parser
converts hexidecimal numbers to decimal inside @code{eq_parser_get_token}
function.

The parser uses grammar rules described in BNF to build subtrees for
expressions, which are combined into a separate tree for each function. BNF
rules are matched inside @code{handle_<rule_name>} functions.
All function trees are stored in @code{function_list} list. Function prototypes
are stored separately in @code{function_proto_list}.

These methods in parser are public:
@itemize
@item
@code{bool eq_parser_init (struct eq_parser *, struct eq_lexer *)} --
initialize parser structure with initial values.

@item
@code{bool eq_parser_finalize (struct eq_parser *)} -- finalize parser
structure and deallocate associated variables.

@item
@code{int eq_parse (struct eq_parser*)} -- parse token sequences and build a
syntax tree for each function and prototype (lists of trees are stored in
@code{function_list} and @code{function_proto_list]} global variables).
@end itemize

@section Tree structure
The tree structure is described in @code{tree.h}. All nodes are members of
@code{union tree_node}. All fundamental types are described in @code{tree.def}.
For each type we define node identifier, string representation, node type,
number of arguments, and either node has a type property. Also there type nodes
which additional properties:
@itemize
@item
@code{struct tree_type_node} -- this node has hash table and list properties.
We use this for storing types. Basically, there is only one instance of each
type only. Therefore, we check type existance using search in hash table.

@item
@code{struct tree_stmt_node} -- Statement node has additional properties which
we use in data flow analysis stage.

@item
@code{struct tree_list_node} -- a list node. This is widely used structure in
compiler for storing all kind of nodes in a list.

@item
@code{struct tree_list_element} -- a single element of a list.

@item
@code{struct tree_function_node} -- a control flow graph is stored inside
function node. Also we store lists of entry points, return statements and print
statements inside function. A schedule represents a list of list of statements
which are possible to execute simultaneously. This are structures used in data
flow analysis and which will be necessary during future translation.

@item
@code{struct tree_circumflex_op_node} -- a circumflex node can be one of two
types. With the circumflex we denote a numerical power operator or recurrence
equation index reference. @code{is_index} field inside circumflex node is true
if this node is related to a recurrence equation.

@item
@code{struct tree_identifier_node} -- in identifier node we store an original
variable name (@code{source_name}). This is necessary, because not all
characters from front-end can be supported by backend language. We also store
reference (@code{iter_def}) to a variable node which stores a description of a
recurrence. @code{iter_desc} is a list of nodes describing recurrence. All the
other fields (@code{def}, @code{ud_chain}, @code{du_chain}) are used during
data flow analysis.

@item
@code{struct tree_iter_pair} -- used to store a relation between index of
recurrence expression and it's definition. In parallel loops
(@code{PARALLEL_LOOP} node) we use also lower indices in the left part of the
statement. To not lose this information we put it into @code{lower_index}
field.

@item
@code{struct tree_hash_node} -- used during single assignment stage. This is a
hash table where we store a list of possible values for phi nodes.

@item
@code{struct tree_phi_node} -- we don't use this node during parsing stage,
however during single assignment stage we insert phi nodes into syntax tree for
data flow analysis.

@item
@code{struct tree_rec_expr_node} -- a description of recurrence expression.
@code{min_value} is the lowest index of a base case for a described recurrence
equation. @code{size} is a number of base cases necessary to describe a
recurrence equation. @code{list} represents a list of all recurrence cases.
@end itemize

Tree node usually is constructed using @code{make_tree} function.

@section Macroprocessor/Matcher
Macroprocessor or matcher transforms a sequence of tokens into a custom
tree. Matcher is implemented as a part of parser module, however it can be used
seperately (with a parser, which works with a different grammar, for example).

A macro substitution can be performed only in place of production of type
@dfn{expr}. In all other cases an error will be produced. A function
@code{perform_transformation} performs a transformation and then
@code{validate_match} is used to check the correspondance to the language
grammar.

The matcher is initialised with the parser (function @code{matcher_init} is 
called from @code{eq_parser_init}). Similarly, @code{matcher_finalize} is
called from @code{eq_parser_finalize}.

Before apply matching transformations, matching rules have to be declared.
According to the grammar, @dfn{\match} productions are to be defined in global
scope before use. Productions consist of left and right part. The left part is an
arbitrary token sequence with a new token which is not in the predefined
token set (a token with a type @dfn{tok_unknown}). The right part of the
production is a sequence of tokens which is supported by the grammar and
corresponds to the @dfn{expr} grammar rule. tree which is inserted in place of
matched token sequence. In this part construction @code{\expr@{ <num> @} } can be
used, which are used as markers. When transformation is performed we substitute
nodes of type @code{EXPR_MATCH} (they represent @code{\expr@{ <num> @} } rules
with actual values provided as macros arguments.

Match expressions are handled in @code{handle_match} function. When both parts
of match expression are parsed, @code{add_match} is called, and a new entry is
added into global hash table called @code{matches}.
@example
struct match_table
@{
  const char *key;
  struct token_list_el *match;
  tree replace;
  UT_hash_handle hh;
@};
@end example
The key (@code{key}) of hash table entry is the literal representation of the first token
(that's why each match should have a unique token as the first one in the
sequence). The second field in entry structure is a token sequence itself
(@code{match}). The third field (@code{replace}) is a tree, which will be
inserted in place where matching token sequence is found.

These matcher methods are public:
@itemize
  @item 
  @code{void matcher_init (void)} -- initialize matcher hash table.
  
  @item
  @code{void add_match (const char *, struct token_list_el *, tree)} -- adds a
  new entry into global hash table.

  @item
  @code{struct match_table *find_match (const char*)} -- find an entry given a
  key. If not found, NULL is returned.

  @item
  @code{tree perform_transform (struct eq_parser *)} -- providing the following
  token sequence relates to some match in @code{matches} table, construct a
  corresponding tree.

  @item
  @code{bool validate_match (struct token_list_el *, tree)} -- validate
  transformation correspondance to grammar rules (we can substitute only
  @dfn{expr} rules). 

  @item
  @code{void print_matches} -- print matcher rules. Can be used for debug
  purpose.
@end itemize

@section Type checking
A static type checker inferences missing types and finds type errors. The
checker recursively proceeds each node of AST starting from the top.

All tree nodes in the AST are separated into two categories: those, who have
type property, and those who don't. This property is defined in
@code{tree.def}. Every typed node has a field @code{TREE_TYPE (node)}, which is
@code{NULL} before type checking. The goal of type checking is to assign a
corresponding type to a @code{TREE_TYPE} field. The information about basic
node types is retrieved from a node identifier and from type
declarations in the source code. Types of more complex expressions are
retrieved using simple inference rules. All types are stored in global table
@code{type_table}. We store only unique types in this table. Therefore, a new
type is added in the table only when it is not found there. Some basic types
are added to the table inside intialization function @code{types_init} (and
removed inside @code{types_finalize} function) -- @code{TYPE_B}, @code{TYPE_N},
@code{TYPE_R}, @code{TYPE_R}. Other types are added to the table using
@code{types_assign_type} function.

The only public method in type checker is:
@itemize
  @item
  @code{int typecheck (void)} -- perform type checking.
@end itemize

@section Control flow analysis

A control flow analysis is performed to build a control flow graph useful for
future optimisations and performing static single assignment.

Inside a public @code{controlflow} function, a control flow analysis is
performed for every function in program using @code{controlflow_function}.

The analysis constructs a graph of @code{basic_block} structures as nodes
connected with @code{edge} edges. Each function has a CFG property which can be
accessed using @code{TREE_FUNC_CFG} macro. The type of the property is
@code{struct control_flow_graph}.

Each basic block contains a variable hash @code{var_hash} for every unique
variable with variable literal string @code{id} as a key. Every hash table
entry contains another hash table itself (@code{struct tree_hash_node})
with phi nodes listed.

The main construction routine is focused in @code{controlflow_pass_block}
function. Here is the brief construction algorithm description:
@enumerate
  @item
  Create a new basic block.

  @item
  We need to link previously created blocks with the current one, if necessary.
  We do it if static varibles @code{join_tail1} and @code{join_tail2} are not
  @code{NULL}.

  @item
  For current statement perform SSA transformation and def-use/use-def chaining using @code{ssa_verify_vars}
  function.

  @item
  Iterate over all statements in function statement list. If it is `if
  statement', then we create a new internal basic block and copy
  (@code{ssa_copy_var_hash} function) existing variable context to the new
  block. Recursively run @code{controlflow_pass_block} function for the
  internal `true predicate body' block. If `false predicate body' exists, then
  do the same for this block as well.

  @item
  After this we need to merge information about variables from the inner
  contexts with the current one. For every variable which was modified inside
  one of inner blocks (we check @code{was_modified} property) we substitute old
  definition with a new one. More precisely:
  @itemize
    @item If variable was modified in both inner blocks, create a phi node
    with definitions from these block. Old definitions from the current block
    are removed.

    @item If variable was modified inside one of inner blocks only, merge
    modified definitions with definitions inside the current block (create a
    new phi node).
  @end itemize

  Store pointers to the new blocks in static variables @code{join_tail1},
  @code{join_tail2}.

  @item
  Move to the next instruction.
@end enumerate

@subsection SSA transformation and Use-Def/Def-Use chaining
Here we describe internals of @code{ssa_verify_vars} function.

Basically, we find all variable occurrences inside a given statement. For each
variable we do a search in a variable hash table inside current basic block for
all possible definitions. Given this information, we do the following:
@itemize
  @item
  Store pointers to @code{IDENTIFIER} nodes of definitions in
  @code{TREE_ID_UD_CHAIN} field for current node.

  @item
  Store a pointer to this @code{IDENTIFIER} node in @code{TREE_ID_DU_CHAIN}
  field of the definition node.

  @item
  For current statement, append statements with definitions of variables which
  are used here (append to @code{TREE_STMT_DEFS} field).

  @item
  For statement with variable definition, append current statement which uses
  defined variable (append to @code{TREE_STMT_USES} field).

  @item
  @code{TREE_ID_DEF} field of the variable node points to the current statement
  node.
@end itemize

All these manipulations set dependency links between variable definitions and
usage to be able to retrieve dataflow information.

Also we do variable redefinitions when we are inside left subtree of the
assignment node. If variable is not found inside basic block's @code{var_hash},
then we simply create a new entry. Otherwise we remove all previous definitions
in the phi node, and create a new one relevant to the current variable.
@section Data flow analysis
After all data dependencies are set, the dataflow analysis becomes
straightforward.

The first thing we do, given a list of return values and print statements for
each function, we find all reachable statements. The remaining statements can
be removed from the AST, as they are not used for computations.

Finally we perform statement scheduling based on the dependencies. We do it
inside @code{dataflow_schedule} function using Breadth First Search (BFS)
algorithm. All statements with the same distance from a root node (root node is
either return or print statement) are combined into a separate list. Such list
represents statements that can be executed concurrently as they don't have
dependencies among them. Finally function @code{dataflow_schedule} returns a
list of lists representing the sequence of sets of statements possible to be
executed in parallel.

@section Code generation

@node Known problems
@chapter Known problems



@node Reporting bugs
@chapter Reporting bugs
If you found a bug or inconsistency in the documentation, please open a new
issue on the github.  In order to do so, pleae use the following web-page:
@url{https://github.com/zayac/eq/issues/}.


@node Getting help
@chapter Getting help
In case you are stuck, please contact one of the authors of the project by
means of github messaging, or by e-mails.  The e-mails are available at the
front page of the project.


@node Contributing to Eq
@chapter Contributing to Eq
If you wish to contribute to the project, then the easiest way would be to
obtain the code, implementa a certain feature, and make a pull-request.

The code is hosted on the github.  In order to just get the code, you can run:
@smallexample
        git clone git@@github.com:zayac/eq.git
@end smallexample

In order to do a pull request, you have to fork a repository, make some
changes on your forked branch, and commit it via pull-request.  The
procedure is well described in git-help:
@url{https://help.github.com/articles/fork-a-repo}.

@node Keyword index
@unnumbered Keyword index
@printindex cp


@node Option index
@unnumbered Option index
@printindex op

@bye
